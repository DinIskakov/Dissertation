{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3393dada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text   Origin           id\n",
      "0  @londonluvspell awesome. I'm on the other side...  Twitter  13570699500\n",
      "1  @JimCarrey \\nIt's never gonna be 106.5 mill ji...  Twitter  13570700102\n",
      "2  Apple: QA Engineer - Summer Internship (Lincol...  Twitter  13570700803\n",
      "3  @Cucuxenxo fue @keyzito , que es un amor e hiz...  Twitter  13570703600\n",
      "4  Iced white mocha! FTW (@ Starbucks w/  @davese...  Twitter  13570706802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1009, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"2010-05-csv/2010-05-07.csv\")\n",
    "print(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3760a663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN: 1009 OUT: 3\n"
     ]
    }
   ],
   "source": [
    "from stage1_subject_filtering.subject_keyword_filtering import filter_subject_keyword_only\n",
    "\n",
    "subject = \"awesome\"\n",
    "df_f = filter_subject_keyword_only(df, subject, text_col=\"Text\")\n",
    "\n",
    "print(\"IN:\", len(df), \"OUT:\", len(df_f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3e3a195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered docs: 3\n",
      "Expanded keywords: ['awesome']\n"
     ]
    }
   ],
   "source": [
    "from stage1_subject_filtering.cooccurence_keyword_filtering import (\n",
    "    filter_subject_cooccurrence_expansion\n",
    ")\n",
    "\n",
    "subject = \"awesome\"\n",
    "TEXT_COL = \"Text\"\n",
    "\n",
    "df_co, keywords = filter_subject_cooccurrence_expansion(\n",
    "    df,\n",
    "    subject,\n",
    "    text_col=TEXT_COL,\n",
    "    top_n=25,\n",
    "    min_count=20,\n",
    ")\n",
    "\n",
    "print(\"Filtered docs:\", len(df_co))\n",
    "print(\"Expanded keywords:\", keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7953157f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic',\n",
       " 'theme',\n",
       " 'matter',\n",
       " 'issue',\n",
       " 'area',\n",
       " 'field',\n",
       " 'discipline',\n",
       " 'study',\n",
       " 'focus',\n",
       " 'content']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import stage1_subject_filtering.llm_expansion as llm_expansion\n",
    "\n",
    "importlib.reload(llm_expansion)\n",
    "synonyms = llm_expansion.get_synonyms(\"subject\")\n",
    "\n",
    "synonyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95bf33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded docs: 1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 16/16 [00:02<00:00,  6.79it/s]\n",
      "2026-01-14 15:47:07,275 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "2026-01-14 15:47:12,473 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-14 15:47:12,473 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-14 15:47:12,508 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-14 15:47:12,510 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-14 15:47:12,527 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved topic info -> topic_info_2010-05-07.csv\n",
      "Saved doc->topic -> doc_topics_2010-05-07.csv\n",
      "Outliers: 6/1009 = 0.006\n",
      "Topics (excluding -1): 7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 1) Load your CSV\n",
    "df = pd.read_csv(\"2010-05-csv/2010-05-07.csv\")\n",
    "\n",
    "TEXT_COL = \"Text\"   \n",
    "\n",
    "if TEXT_COL not in df.columns:\n",
    "    raise KeyError(f\"'{TEXT_COL}' not found. Columns: {list(df.columns)}\")\n",
    "\n",
    "texts = df[TEXT_COL].fillna(\"\").astype(str).tolist()\n",
    "texts = [t for t in texts if t.strip()]\n",
    "print(\"Loaded docs:\", len(texts))\n",
    "\n",
    "# 2) Embedding model (fast/light)\n",
    "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = encoder.encode(\n",
    "    texts,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=64,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "\n",
    "# 3) UMAP (fixed hyperparams)\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=5,\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# 4) HDBSCAN (fixed hyperparams)\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=15,\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data=True,\n",
    ")\n",
    "\n",
    "# 5) Vectorizer (for c-TF-IDF keywords)\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    ")\n",
    "\n",
    "# 6) BERTopic\n",
    "topic_model = BERTopic(\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(texts, embeddings)\n",
    "\n",
    "# 7) Save outputs\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info.to_csv(\"topic_info_2010-05-07.csv\", index=False)\n",
    "print(\"Saved topic info -> topic_info_2010-05-07.csv\")\n",
    "\n",
    "doc_topics = pd.DataFrame({\"text\": texts, \"topic\": topics})\n",
    "doc_topics.to_csv(\"doc_topics_2010-05-07.csv\", index=False)\n",
    "print(\"Saved doc->topic -> doc_topics_2010-05-07.csv\")\n",
    "\n",
    "# 8) Quick sanity stats\n",
    "n_outliers = sum(1 for t in topics if t == -1)\n",
    "print(f\"Outliers: {n_outliers}/{len(texts)} = {n_outliers/len(texts):.3f}\")\n",
    "print(\"Topics (excluding -1):\", (topic_info[\"Topic\"] != -1).sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
